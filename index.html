<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Massive Datenströme mit Kafka</title>

	<meta name="description" content="data2day 2015 - Massive Datenströme mit Kafka">
	<meta name="author" content="Frank Wisniewski & Lars Pfannenschmidt">

	<meta name="apple-mobile-web-app-capable" content="yes" />
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

	<link rel="stylesheet" href="css/reveal.css">
	<link rel="stylesheet" href="css/theme/simple.css" id="theme">

	<!-- Code syntax highlighting -->
	<link rel="stylesheet" href="lib/css/zenburn.css">

	<!-- Printing and PDF exports -->
	<script>
	var link = document.createElement( 'link' );
	link.rel = 'stylesheet';
	link.type = 'text/css';
	link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
	document.getElementsByTagName( 'head' )[0].appendChild( link );
	</script>

	<!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
</head>

<body>
	<div class="reveal">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<section>
				<small><h3>data2day 2015</h3></small>
				<h1>Massive Datenströme mit Kafka</h1>
				<p>
					<small><a href="https://github.com/frankwis">Frank Wisniewski</a> - <a href="http://twitter.com/ultraknackig">@ultraknackig</a><br>
						<a href="https://github.com/larsp">Lars Pfannenschmidt</a> - <a href="http://twitter.com/leastangle">@leastangle</a></small>
					</p>
					<aside class="notes">
						<ul>
							<li>Wer hat schon mal mit andere Message Brokern wie RabbitMQ, ActiveMQ oder vielleicht sogar mit Verbrechen wie JMS gearbeitet?</li>
							<li>Wer hat schon mal mit Kafka gearbeitet?</li>
							<li>F&uuml;r die, die es nicht wissen. Kafka ist ein...</li>
							<ul>
								<li>verteilter</li>
								<li>partitionierender</li>
								<li>replizierender</li>
							</ul>
							...Service f&uuml;r Datenstr&ouml;me.
						</ul>
					</aside>
				</section>

				<section>
					<h2>$ whoami</h2>
					<h4>Frank Wisniewski</h4>
					<ul>
						<li>Real Time Data Products @Intuit</li>
						<li>Founder of <a href="https://twitter.com/mobilecgn">@mobilecgn</a> User Group</li>
						<li><a href="https://twitter.com/leastangle">@leastangle</a></li>
					</ul>

					<p/>

					<h4>Lars Pfannenschmidt</h4>
					<ul>
						<li>Real Time Data Products @Intuit</li>
						<li>Founder of <a href="https://twitter.com/mobilecgn">@mobilecgn</a> User Group</li>
						<li><a href="https://twitter.com/leastangle">@leastangle</a></li>
					</ul>

					<aside class="notes">
						Hallo, mein Name ist XXX. Mehr zu uns findet ihr auf den Folien.
					</aside>
				</section>

				<section>
					<h2>$ ls -al</h2>
					<ul>
						<li class="fragment">Motivation</li>
						<div class="fragment">
							<li>Concepts</li>
							<ul>
								<li>Topics &amp; Partitions</li>
								<li>Offset Management</li>
								<li>Log Compaction</li>
							</ul>
						</div>
						<div class="fragment">
							<li>Example</li>
							<ul>
								<li>Producer</li>
								<li>Consumer</li>
							</ul>
						</div>
						<li class="fragment">Performance</li>
						<li class="fragment">Summary</li>
					</ul>
					<aside class="notes">
						Concepts tauchen in der Agenda auf, weil sie sich grund&auml;tzlich von den klassischen Brokern unterscheiden.
						Um euch das angewandt n&auml;her zu bringen, haben wir euch ein Anwendungsbeispiel in Java mitgebracht.
						Performance: Warum ist Kafka in der Lage derartige Datenmengen zu verarbeiten und was ist ggfls. zu beachten.
					</aside>
				</section>

				<section>
					<section data-background="img/motivation.jpg">
						<h1>Motivation</h1>
						<small><a href="https://flic.kr/p/2k2Ww3">high wire 2</a> by Graeme Maclean - Some rights reserved <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></small>
					</section>
					<section>
						<h2>Chaos</h2>
						<img src="img/datapipeline_complex.jpg"/>
						<small><a href="http://linkd.in/199iMwY">„The Log: What every software engineer should know about real­time data's unifying abstraction“</a> by Jay Kreps</small>
					</section>
					<section>
						<h2>Order</h2>
						<img src="img/datapipeline_simple.jpg"/>
						<small><a href="http://linkd.in/199iMwY">„The Log: What every software engineer should know about real­time data's unifying abstraction“</a> by Jay Kreps</small>
					</section>
				</section>

				<section>
					<section data-background="img/concepts.jpg">
						<h1>Concepts</h1>
						<small><a href="https://flic.kr/p/iRs3gB">1960 Lloyd Arabella</a> by JOHN LLOYD - Some rights reserved <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></small>
					</section>
					<section>
						<h2>Overview</h2>
						<img width="65%" src="img/Abbildung 1 - Übersicht.png"/>
						<small>Topological Overview according to <a href="http://kafka.apache.org/documentation.html">Kafka documentation</a></small>
					</section>
					<section>
						<h2>Topics &amp; Partitions</h2>
						<img width="65%" src="img/Abbildung 2 - Topic.png"/>
						<small>Anatomy of a topic according to <a href="http://kafka.apache.org/documentation.html">Kafka documentation</a></small>
						<aside class="notes">
							Nachrichten werden Topics in organisiert
							Ein Topic n Partitionen (auf unterschiedlichen Knoten) verteilt werden
							Pro Partition werden die Message sequentiell weggeschrieben.
							Offset: Fortlaufende Nummer je Partition.
							Replikation: Jedes Topic l&auml;sst sich n fach replizieren
							Die Verweildauer einer Nachricht l&auml;sst sich &uuml;ber die Retention festlegen
						</aside>
					</section>
					<section>
						<h2>Guarantees</h2>
						<ul>
							<li>Messages sent to a particular partition will be appended in the order they are seen</li>
							<li>Consumer sees messages in the order they are stored in the log</li>
							<li>Kafka will tolerate up to N-1 server failures for a topic with replication factor N</li>
						</ul>
					</section>
					<section>
						<h2>Log Compaction</h2>
						<img width="65%" src="img/Abbildung 3 - LogCompaction.png"/>
						<small>Log compaction in conformity with <a href="http://kafka.apache.org/documentation.html">Kafka documentation</a></small>
						<aside class="notes">
							Compression nicht m&ouml;glich mit Log Compaction
						</aside>
					</section>
				</section>

				<section>
					<section data-background="img/examples.jpg">
						<h1>Example</h1>
						<small><a href="https://flic.kr/p/mHwYY">Hello World</a> by Bill Bradford - Some rights reserved <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></small>
					</section>
					<section>
						<h2>Java Producer</h2>
						<pre class="fragment"><code data-trim class="java">
public class News {
    public final UUID id;
    public final String author, title, body;
...
}
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
Properties config = new Properties();
config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, broker);
config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, NewsSerializer.class.getName());

KafkaProducer&lt;String, News&gt; producer = new KafkaProducer&lt;&gt;(config);
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
public RecordMetadata send(News news) throws ExecutionException, InterruptedException {
	ProducerRecord&lt;String, News&gt; record = new ProducerRecord&lt;&gt;(topic, news.id.toString(), news);
	Future&lt;RecordMetadata&gt; recordMetadataFuture = this.producer.send(record);
	return recordMetadataFuture.get();
}
						</code></pre>
						<aside class="notes">
							Alles was durch Kafka geht, sind Byte-Arrays. Wenn man also in seiner Dom&auml;ne etwas anderes verwenden m&ouml;chte, dann m&uuml;ssen eigene <b>Serializer</b> geschrieben werden ;-)
						</aside>
					</section>
					<section>
						<h2>Message Distribution</h2>
						Control and configure message routing via <code>ProducerRecord</code>:<br /><br />
						<ul>
							<li class="fragment">
								Use round-robin to choose target partition
								<pre ><code data-trim class="java">
									ProducerRecord(String topic, V value);
								</code></pre>
							</li>
							<li class="fragment">
								Assign identic partition using key hash
								<pre class="fragment"><code data-trim class="java">
									ProducerRecord(String topic, K key, V value);
								</code></pre>
							</li>
							<li class="fragment">
								Specify partition explicitly
								<pre class="fragment"><code data-trim class="java">
									ProducerRecord(String topic, Integer partition, K key, V value);
								</code></pre>
							</li>
						</ul>
						<aside class="notes">
							<b>Erinnerung:</b> Reihenfolge wird pro Partititon garantiert
							<ul>
								<li>Ohne Key hat <b>Log Compaction</b> keinen sinnvollen effekt</li>
								<li>Objekte mit gleichem <b>Key</b> landen <i>immer</i> in gleicher Partition &rarr; Garantie f&uuml;r Reihenfolge (bspw. f&uuml;r Event-Streams)</li>
								<li>Partitionierung an Hand von anderem Merkmal bzw. Identifer als dem Record-Key m&ouml;glich &rarr; Log-Compaction und Verteilung &uuml;ber seperate Keys</li>
							</ul>
							<b>Obacht:</b> Nachtr&auml;gliches &Auml;ndern der Partitionsgr&ouml;ße, re-partiitoniert nicht vorhandene Nachrichten
						</aside>
					</section>
					<section>
						<h2>Consumer Grouping</h2>
						<img width="65%" src="img/Abbildung 6 - ConsumerGrouping.png"/>
						<small>“Kafka 101 - Massive Datenströme mit Apache Kafka” by Pfannenschmidt and Wisniewski, JavaMagazin 08.2015, p. 38</small>
					</section>
					<aside class="notes">
						Strategie zum Konsumieren der Daten.
						Klassisch: Einstellung auf dem Broker, hier &uuml;ber die Art des Auslesens
						links queue, rechts publish subscribe
					</aside>
					<section>
						<h2>Consumer Threading</h2>
						<img width="65%" src="img/Abbildung 8 - ConsumerThreading.png"/>
						<small>“Kafka 101 - Massive Datenströme mit Apache Kafka” by Pfannenschmidt and Wisniewski, JavaMagazin 08.2015, p. 41</small>
					</section>
					<section>
						<h2>Java Consumer Thread</h2>
						<pre class="fragment"><code data-trim class="java">
public void run() {
	Thread.currentThread().setName(name);
	ConsumerIterator&lt;String, News&gt; it = messageStream.iterator();
	while (it.hasNext()) {
		relayMessage(it.next());
	}
}
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
void relayMessage(MessageAndMetadata&lt;String, News&gt; kafkaMessage) {
    logger.trace("Received message with key '{}'"
	    + " and offset '{}' on partition '{}' for topic '{}'",
	    kafkaMessage.key(), kafkaMessage.offset(),
	    kafkaMessage.partition(), kafkaMessage.topic());
	messageConsumer.consume(kafkaMessage.message());
}
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
public interface NewsConsumer&lt;News&gt; {
	void consume(News message);
}
						</code></pre>
						<aside class="notes">
							<ul>
								<li>High Level Consumer API, SimpleConsumer + Client Re-Design</li>
							</ul>
						</aside>
					</section>
					<section>
						<h2>Wiring</h2>
						<pre class="fragment"><code data-trim class="java">
import static kafka.consumer.Consumer.createJavaConsumerConnector;

Properties props = new Properties();
props.put("zookeeper.connect", zookeeper); // list of ZooKeeper nodes
props.put("group.id", groupId);            // identifies consumer group
props.put("offsets.storage", "kafka");     // storage for offsets
props.put("dual.commit.enabled", "false"); // migration switch
...
ConsumerConnector consumerConnector = createJavaConsumerConnector(new ConsumerConfig(props));
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
Map&lt;String, List&lt;KafkaStream&lt;String, News&gt;&gt;&gt; consumerMap;
consumerMap = consumerConnector.createMessageStreams(
    ImmutableMap.of(topic, numberOfThreads),     // number of streams per topic
    new StringDecoder(null), new NewsDecoder()); // message key and value decoders

List&lt;KafkaStream&lt;String, News&gt;&gt; streams = consumerMap.get(topic);
						</code></pre>
						<pre class="fragment"><code data-trim class="java">
// create fixed size thread pool to launch all the threads
ExecutorService pool = Executors.newFixedThreadPool(numberOfThreads);
// create consumer threads to handle the messages
for (final KafkaStream stream : streams) {
	String name = String.format("%s[%s]", topic, threadNumber++);
	pool.submit(new ConsumerThread(stream, name, consumer));
}
						</code></pre>
						<aside class="notes">
							Anmerkung: Gott sei Dank ist eine neue Java-API in Arbeit!
						</aside>
					</section>
				</section>

				<section>
					<section data-background="img/performance.jpg">
						<h1>Performance</h1>
						<small><a href="https://flic.kr/p/wQgqtx">Grefsenkollen Downhill 2015</a> by Vegar Nilsen - Some rights reserved <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></small>
					</section>
					<section>
						<h2>What makes Kafka fast?</h2>
						<h4>Designed to make consumption as cheap as possible</h4>
						<h4>Fast Writes:</h4>
						<ul>
							<li>linear writes</li>
							<li>all writes go to OS pagecache</li>
						</ul>
						<p/>
						<h4>Fast Reads:</h4>
						<ul>
							<li>linear reads</li>
							<li>direct data transport from <br>pagecache to socket via <code><a href="http://man7.org/linux/man-pages/man2/sendfile.2.html" target="blank">sendfile()</a></code></li>
							<li>Consumer without lag get all<br>messages from pagecache</li>
						</ul>
						<br><br>
						<h3>Combined -> Fast!</h3>
						<p>
							<small>More Details: <a href="http://kafka.apache.org/documentation.html#persistence" target="_blank">http://kafka.apache.org/documentation.html#persistence</a><br/></small>
						</p>
						<aside class="notes">
							- Durable queues in classic systems can be very slow.
							- linear writes outperform random writes by far, highly optimized in OS.
							https://www.ibm.com/developerworks/linux/library/j-zerocopy/
						</aside>
						</section>
						<section>
							<h2>sendfile()?</h2>
							<img width="80%" src="img/traditional data copy vs sendfile.png"/>
							<small>Efficient data transfer according to  <a href="http://www.ibm.com/developerworks/library/j-zerocopy/">http://www.ibm.com/developerworks/library/j-zerocopy/</a></small>
							<aside class="notes">
								In Java sorgt <code>FileChannel.transferTo()</code>, falls es die JVM des OS unterst&uuml;tzt, daf&uuml;r, dass sendfile() benutzt wird.
							</aside>
						</section>
					</section>
				</section>
				<section>
					<section>
						<h1>Summary</h1>
					</section>
					<section>
						<h2>Key Features &amp; Facts</h2>
						<ul>
							<li class="fragment"><b>Commit Log</b>&emsp;&emsp;Topics, Partitioning, Log Compaction, Compression &amp; Offset Management</li>
							<li class="fragment"><b>Real-time</b>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;High-Throughput &amp; Low-Latency</li>
							<li class="fragment"><b>Persistence</b>&emsp;&emsp;&emsp;Scalable, centralized &amp; replicated storage</li>
						</ul>
					</section>
					<section>
						<h2>Related</h2>
						<ul>
							<li class="fragment"><b>Clients</b>&emsp;&emsp;Scala, Java, Python, Go, Perl, Erlang etc.</li>
							<li class="fragment"><b>Integration</b>&emsp;&emsp;Storm, Samza, Hadoop, ElasticSearch, Logstash, Hive etc.</li>
							<li class="fragment"><b>Confluent Platform</b>&emsp;&emsp;Schema Registry, REST Proxy &amp; Camus</li>
						</ul>
						<aside class="notes">
							Integrates with Stream Processing (Storm, Samza), Search (ElasticSearch), Logging und Metrics.
						</aside>
					</section>
				</section>
				<section>
					<h1>Thank You!</h1>
					<p>
						<a href="http://datafreaks.io">datanerds.io</a> /
						<a href="https://github.com/kafka101">github.com/kafka101</a>
					</p>
				</section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

		// Full list of configuration options available at:
		// https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			history: true,
			center: true,
			width: 1280,
			height: 1024,
			margin: 0.1,
			minScale: 0.8,
			maxScale: 1.2,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// Optional reveal.js plugins
			dependencies: [
				{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
				{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
				{ src: 'plugin/highlight/highlight.js', async: true, condition: function() { return !!document.querySelector( 'pre code' ); }, callback: function() { hljs.initHighlightingOnLoad(); } },
				{ src: 'plugin/zoom-js/zoom.js', async: true },
				{ src: 'plugin/notes/notes.js', async: true }
			]
		});

		</script>

	</body>
	</html>
